{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"HW_lessons_9-10.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgxdX4L0dSw7"
      },
      "source": [
        "# Домашнее задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piodEhR7orUF"
      },
      "source": [
        "Ноутбук в колаб: https://colab.research.google.com/drive/1d-vvpJW8IWSPeodFnfaEVaZIQrys8uU0?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3_T-pcpnme8"
      },
      "source": [
        "**Дедлайн: 01.01.2021, 23:59**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2RmFtRDn5vy"
      },
      "source": [
        "Формат отчетности - jupyter notebook. Однако вычислять производные не обязательно в Markdown. Если вычисляете вручную, то дополнительно с ноутбуком, отправляйте pdf-файл с расписанным решением."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCDx6dKzeEhp"
      },
      "source": [
        "### Пример реализации градиентного спуска: https://github.com/ddvika/Data-Science-School-2020/blob/main/lecture_9/gradient_methods.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-G1btbwoStu"
      },
      "source": [
        "# Задания"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1XambpmoeIj"
      },
      "source": [
        "Перед выполнением ДЗ посмотрите на ноутбук, прикрепленный по ссылке выше. Там вы найдете реализацию градиентного спуска с постоянным и дробным шагом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnSTXgrueAp0"
      },
      "source": [
        "### Задание 1. [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XmQ1AO9f1Qz"
      },
      "source": [
        "Релизуйте градиентный спуск с постоянным шагом и с дробным шагом для функции\n",
        "$$\n",
        "y = x_{1}^{2}+5 x_{2}^{2}\n",
        "$$\n",
        "\n",
        "в произвольно выбранной Вами точке."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAXdL_PDgYpS"
      },
      "source": [
        "Поэксперементируйте с разными значениями шага (скорости обучения), попробуйте хотя бы по 2-3 разных значения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucbvMBsdgOwV"
      },
      "source": [
        "### Задание 2. [3 points]\n",
        "\n",
        "Для функции из предыдущего задания реализуйте градиентный спуск, в котором значение шага (скорости обучения) будет изменяться по формуле циклического косинусного ожига. ( в англ. литературе - cosine annealing learning rate или cosine decay lr). \n",
        "\n",
        "Доп. литература:\n",
        "- Циклический косинусный отжиг https://habr.com/ru/post/332534/\n",
        "\n",
        "- Пример colise decay в библиотеке Pytorch:\n",
        "https://www.programmersought.com/article/12164650026/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2Non6JiiVoJ"
      },
      "source": [
        "### Задание 3. [0.75 point]\n",
        "Проверьте работу Вашего градиентного спуска с косинусным отжигом на произвольной функции ( полином должен быть не меньше 3-ьего порядка и задан в пространстве не меньше $R^3$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpONErtji81D"
      },
      "source": [
        "### Задание 4.\n",
        "Случайная величина X задана следующей функцией распределения:\n",
        "$$\n",
        "f(x)=\\left\\{\\begin{array}{r}\n",
        "0 \\text { npu } x \\leq \\pi \\\\\n",
        "-\\cos x \\text { npu } \\pi<x \\leq \\frac{3}{2} \\pi \\\\\n",
        "\\text { 0 npu } x>\\frac{3}{2} \\pi\n",
        "\\end{array}\\right.\n",
        "$$\n",
        "\n",
        "1. Постройте данную функцию распределения при помощи библиотеки seaborn **[0.25 point]**\n",
        "2. Найдите плотность вероятности. **[1 point]**\n",
        "3. Постройте график полученной плотности вероятности **[0.25 point]**\n",
        "\n",
        "4. Определить вероятность попадания случайной величины X в интервал $\\left[\\pi, \\frac{5}{4} \\pi\\right]$ **[0.75 point]**\n",
        "\n",
        "5. Найти математическое ожидание и дисперсию случайной величины X . **[0.75 point]**\n",
        "\n",
        "Так как мы не проходили интегрирование, то в 4 и 5 пунктах можете использовать\n",
        "wolfram alpha (https://www.wolframalpha.com) для интегрирования. Однако 2ой пункт задания (на нахождение производной) должен быть расписан!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LrqILMHkgTd"
      },
      "source": [
        "### Задание 5 [1.5 point]\n",
        "\n",
        " Случайная величина Х задана функцией распределения F(x).\n",
        "\n",
        " $$F(x)=\\left\\{\\begin{array}{c}0, x \\leq 1 \\\\ x-1,1<x \\leq 2 \\\\ 1, x>2\\end{array}\\right.$$\n",
        "\n",
        " 1. Является ли случайная величина Х непрерывной?\n",
        "\n",
        " 2. имеет ли случайная величина Х плотность вероятности f(X)? Если имеет, найти ее. \n",
        " 3. постройте графики f(X) и F(X), если такое возможно.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzQy9iYWmGD5"
      },
      "source": [
        "### Задание 6\n",
        "\n",
        "Рассмотрим несбалансированный набор данных с соотношением меньшего класса к большему 1: 100, где 100 экземпляров принадлежит меньшему классу, а 10 000 большему.\n",
        "\n",
        "Модель ML делает прогнозы и предсказывает 120 примеров как принадлежащих к классу меньшинства, 90 из которых верны, а 30 - неверны.\n",
        "\n",
        "Найти:\n",
        "\n",
        "- Precision **[0.5 point]**\n",
        "- Recall **[0.5 point]**\n",
        "- $F_1$ метрику **[0.5 point]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UM3W_2t_je-"
      },
      "source": [
        "#Задание №1\r\n",
        "#Дана функция:\r\n",
        "$$\r\n",
        "y = x_{1}^{2}+5 x_{2}^{2}\r\n",
        "$$\r\n",
        "#Производная:\r\n",
        "$$\r\n",
        "f'(x) = 2 x_{1} + 10 x_{2}\r\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnkSjI5tdWJe"
      },
      "source": [
        "import numpy as np\r\n",
        "def f(x):\r\n",
        "    return x[0]**2+ 5*x[1]**2\r\n",
        "    \r\n",
        "def grad_f(x):\r\n",
        "    return np.array([2*x[0], 10*x[1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mye_Q-jgdP0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ebe406-8fd5-4f27-e1cf-9d450b5993cf"
      },
      "source": [
        "#Градиентный спуск с постоянным шагом \r\n",
        "def grad_descent_const_step(x = np.array([1, 1]), alpha = 0.001, epsilon = 0.05):\r\n",
        "    grad = grad_f(x)\r\n",
        "    n = 0\r\n",
        "    check = 0\r\n",
        "    while (np.linalg.norm(grad) > epsilon) or (check < 3):\r\n",
        "        x = x - alpha*grad\r\n",
        "        grad = grad_f(x)\r\n",
        "        n+=1\r\n",
        "        if (np.linalg.norm(grad) <= epsilon): check +=1\r\n",
        "    print(\"Градиентный спуск с постоянным шагом выполнил {} шагов\".format(n))\r\n",
        "    print(\"Точка с координатами х1 = {}, x2 = {}\".format(x[0], x[1]))\r\n",
        "    return x\r\n",
        "\r\n",
        "x = grad_descent_const_step(alpha = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Градиентный спуск с постоянным шагом выполнил 19 шагов\n",
            "Точка с координатами х1 = 0.014411518807585589, x2 = 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze0rLS36EXWs",
        "outputId": "7c25bdba-123a-4c30-f96c-9117ad12a8f8"
      },
      "source": [
        "#Градиентный спуск с заранее заданным шагом\r\n",
        "def grad_descent_series(x = np.array([1, 1]), epsilon = 0.05):\r\n",
        "    grad = grad_f(x)\r\n",
        "    n = 0\r\n",
        "    k = 1 \r\n",
        "    check = 0\r\n",
        "    while np.linalg.norm(grad) > epsilon or check < 3:\r\n",
        "        x = x - (1/k)*grad\r\n",
        "        grad = grad_f(x)\r\n",
        "        k+=1\r\n",
        "        n+=1\r\n",
        "        if (np.linalg.norm(grad) <= epsilon): check +=1\r\n",
        "    print(\"Градиентный спуст с заранее заданным шагом выполнил {} шагов\".format(n))\r\n",
        "    print(\"Точка с координатами х1 = {}, x2 = {}\".format(x[0], x[1]))\r\n",
        "    return x\r\n",
        "a = grad_descent_series()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Градиентный спуст с заранее заданным шагом выполнил 12 шагов\n",
            "Точка с координатами х1 = 0.0, x2 = 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VdNfXvoEw4q",
        "outputId": "cf970ae1-8041-443e-c815-d6f6e8d5115c"
      },
      "source": [
        "#Градиентный спуск с дроблением шага\r\n",
        "def grad_descent_step_splitting(x = np.array([1, 1]), alpha = 1, epsilon = 0.05, ksi = 0.5, lambda_d = 0.35):\r\n",
        "    grad = grad_f(x)\r\n",
        "    n = 0\r\n",
        "    n_alpha = 0\r\n",
        "    alpha_k = alpha\r\n",
        "    x_k0 = x\r\n",
        "    check = 0\r\n",
        "    while np.linalg.norm(grad) > epsilon or check < 3:\r\n",
        "        grad = grad_f(x_k0)\r\n",
        "        x_k1 = x_k0 - alpha_k*grad\r\n",
        "        while f(x_k1) - f(x_k0) > - alpha_k * ksi * (np.linalg.norm(grad)**2):\r\n",
        "            alpha_k *= lambda_d\r\n",
        "            x_k1 = x_k0 - alpha_k*grad\r\n",
        "            n_alpha+=1\r\n",
        "        x_k0 = x_k0 - alpha_k*grad\r\n",
        "        alpha_k = alpha\r\n",
        "        n+=1\r\n",
        "        if (np.linalg.norm(grad) <= epsilon): check +=1\r\n",
        "    x = x_k0\r\n",
        "    print(\"Градиентный спуст с дроблением шага выполнил {} шагов\".format(n))\r\n",
        "    print(\"Выполнено {} итераций дробления шага\".format(n_alpha))\r\n",
        "    print(\"Точка с координатами х1 = {}, x2 = {}\".format(x[0], x[1]))\r\n",
        "    return x\r\n",
        "a = grad_descent_step_splitting()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Градиентный спуст с дроблением шага выполнил 13 шагов\n",
            "Выполнено 27 итераций дробления шага\n",
            "Точка с координатами х1 = 0.0034938588036157476, x2 = -0.000215883766723314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6QHbV90FzuM",
        "outputId": "da706ccb-c59d-4c1d-c911-1a87667e2426"
      },
      "source": [
        "a, b, eps = 0.0001, 0.9999,  0.005\r\n",
        "def passive_search(x, grad, a = a, b = b, eps = eps):\r\n",
        "    \r\n",
        "    n = round((b-a)/eps)+1\r\n",
        "    x_s = [a+i*eps for i in range(n)]\r\n",
        "    y_s = [f(x-i*grad) for i in x_s]\r\n",
        "    res = y_s.index(min(y_s))\r\n",
        "    return x_s[res]\r\n",
        "def grad_descent_series(x = np.array([1, 1]), epsilon = 0.05):\r\n",
        "    grad = grad_f(x)\r\n",
        "    n = 0\r\n",
        "    check = 0\r\n",
        "    while np.linalg.norm(grad) > epsilon or check < 3:\r\n",
        "        alp = passive_search(x, grad)\r\n",
        "        x = x - alp*grad\r\n",
        "        grad = grad_f(x)\r\n",
        "        n+=1\r\n",
        "        if (np.linalg.norm(grad) <= epsilon): check +=1\r\n",
        "    print(\"Метод наискорейшего градиентного спуска выполнил {} шагов\".format(n))\r\n",
        "    print(\"Точка с координатами х1 = {}, x2 = {}\".format(x[0], x[1]))\r\n",
        "    return x\r\n",
        "a = grad_descent_series()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Метод наискорейшего градиентного спуска выполнил 9 шагов\n",
            "Точка с координатами х1 = 0.004351633122739961, x2 = -0.0004135186079122933\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}